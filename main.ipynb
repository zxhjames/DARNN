{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.2)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.4.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Tuple\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from modules import Encoder, Decoder\n",
    "from custom_types import DaRnnNet, TrainData, TrainConfig\n",
    "from utils import numpy_to_tvar\n",
    "from constants import device\n",
    "\n",
    "logger = utils.setup_log()\n",
    "logger.info(f\"Using computation device: {device}\")\n",
    "\n",
    "\n",
    "'''\n",
    "数据预处理\n",
    "'''\n",
    "def preprocess_data(dat, col_names) -> Tuple[TrainData, StandardScaler]:\n",
    "    # 标准化\n",
    "    scale = StandardScaler().fit(dat)\n",
    "    proc_dat = scale.transform(dat)\n",
    "\n",
    "    # 生成同等列长的mask数组\n",
    "    mask = np.ones(proc_dat.shape[1], dtype=bool)\n",
    "    dat_cols = list(dat.columns)\n",
    "    for col_name in col_names:\n",
    "        mask[dat_cols.index(col_name)] = False\n",
    "    print(type(proc_dat))\n",
    "    feats = proc_dat[:, mask]\n",
    "    targs = proc_dat[:, ~mask]\n",
    "    # TrainData是一个对象，里面包含了特征features和标签targets\n",
    "    return TrainData(feats, targs), scale\n",
    "\n",
    "'''\n",
    "构建网络\n",
    "'''\n",
    "def da_rnn(train_data: TrainData, n_targs: int, encoder_hidden_size=64, decoder_hidden_size=64,\n",
    "           T=10, learning_rate=0.01, batch_size=128):\n",
    "\n",
    "    #定义配置器 T=>滑窗长度 截取前70%的数据作为训练集\n",
    "    train_cfg = TrainConfig(T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\n",
    "    logger.info(f\"Training size: {train_cfg.train_size:d}.\")\n",
    "    enc_kwargs = {\"input_size\": train_data.feats.shape[1], \"hidden_size\": encoder_hidden_size, \"T\": T}\n",
    "    encoder = Encoder(**enc_kwargs).to(device)\n",
    "\n",
    "    # 将encoder层，decoder层配置写入配置文件\n",
    "    with open(os.path.join(\"data\", \"enc_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(enc_kwargs, fi, indent=4)\n",
    "\n",
    "    dec_kwargs = {\"encoder_hidden_size\": encoder_hidden_size,\n",
    "                  \"decoder_hidden_size\": decoder_hidden_size, \"T\": T, \"out_feats\": n_targs}\n",
    "    decoder = Decoder(**dec_kwargs).to(device)\n",
    "    with open(os.path.join(\"data\", \"dec_kwargs.json\"), \"w\") as fi:\n",
    "        json.dump(dec_kwargs, fi, indent=4)\n",
    "\n",
    "\n",
    "    encoder_optimizer = optim.Adam(\n",
    "        params=[p for p in encoder.parameters() if p.requires_grad],\n",
    "        lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(\n",
    "        params=[p for p in decoder.parameters() if p.requires_grad],\n",
    "        lr=learning_rate)\n",
    "    da_rnn_net = DaRnnNet(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "\n",
    "    return train_cfg, da_rnn_net\n",
    "\n",
    "#TODO模型训练\n",
    "'''\n",
    "DaRnnNet 网络架构\n",
    "train_data 训练集\n",
    "t_cfg 训练集配置\n",
    "n_epochs 迭代次数\n",
    "save_plots 是否保存图片\n",
    "'''\n",
    "def train(net: DaRnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs=10, save_plots=False):\n",
    "    # 向上取整\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\n",
    "    # 存储迭代损失值\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\n",
    "    # 存储每轮epoch的损失值\n",
    "    epoch_losses = np.zeros(n_epochs)\n",
    "    print('iter_loss:',iter_losses)\n",
    "    print('epoch_loss: ',epoch_losses)\n",
    "    logger.info(f\"Iterations per epoch: {t_cfg.train_size * 1. / t_cfg.batch_size:3.3f} ~ {iter_per_epoch:d}.\")\n",
    "\n",
    "    n_iter = 0\n",
    "\n",
    "    print('一共',n_epochs,'次迭代')\n",
    "    for e_i in range(n_epochs):\n",
    "        print('现在是第 ',e_i,'轮迭代')\n",
    "\n",
    "        # 随机生成\n",
    "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\n",
    "        # 循环迭代 每次迭代的步长为batch_size\\\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\n",
    "            # 随机采样\n",
    "            batch_idx = perm_idx[t_i:(t_i + t_cfg.batch_size)]\n",
    "            # 滑窗策略\n",
    "            feats, y_history, y_target = prep_train_data(batch_idx, t_cfg, train_data)\n",
    "            # 计算loss值\n",
    "            loss = train_iteration(net, t_cfg.loss_func, feats, y_history, y_target)\n",
    "            iter_losses[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = loss\n",
    "            # if (j / t_cfg.batch_size) % 50 == 0:\n",
    "            #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / t_cfg.batch_size, loss)\n",
    "            n_iter += 1\n",
    "\n",
    "            adjust_learning_rate(net, n_iter)\n",
    "\n",
    "        epoch_losses[e_i] = np.mean(iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\n",
    "\n",
    "        if e_i % 10 == 0:\n",
    "            y_test_pred = predict(net, train_data,\n",
    "                                  t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                  on_train=False)\n",
    "            # TODO: make this MSE and make it work for multiple inputs\n",
    "            val_loss = y_test_pred - train_data.targs[t_cfg.train_size:]\n",
    "            logger.info(f\"Epoch {e_i:d}, train loss: {epoch_losses[e_i]:3.3f}, val loss: {np.mean(np.abs(val_loss))}.\")\n",
    "            y_train_pred = predict(net, train_data,\n",
    "                                   t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\n",
    "                                   on_train=True)\n",
    "            plt.figure()\n",
    "            plt.plot(range(1, 1 + len(train_data.targs)), train_data.targs,\n",
    "                     label=\"True\")\n",
    "            plt.plot(range(t_cfg.T, len(y_train_pred) + t_cfg.T), y_train_pred,\n",
    "                     label='Predicted - Train')\n",
    "            plt.plot(range(t_cfg.T + len(y_train_pred), len(train_data.targs) + 1), y_test_pred,\n",
    "                     label='Predicted - Test')\n",
    "            plt.legend(loc='upper left')\n",
    "            utils.save_or_show_plot(f\"pred_{e_i}.png\", save_plots)\n",
    "\n",
    "    return iter_losses, epoch_losses\n",
    "\n",
    "\n",
    "def prep_train_data(batch_idx: np.ndarray, t_cfg: TrainConfig, train_data: TrainData):\n",
    "    feats = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.feats.shape[1]))\n",
    "    y_history = np.zeros((len(batch_idx), t_cfg.T - 1, train_data.targs.shape[1]))\n",
    "    y_target = train_data.targs[batch_idx + t_cfg.T]\n",
    "    # 获取采样的batch_id的下标和值\n",
    "    # 获取特征和标签的相应下标值\n",
    "    for b_i, b_idx in enumerate(batch_idx):\n",
    "        b_slc = slice(b_idx, b_idx + t_cfg.T - 1)\n",
    "        feats[b_i, :, :] = train_data.feats[b_slc, :]\n",
    "        y_history[b_i, :] = train_data.targs[b_slc]\n",
    "\n",
    "    return feats, y_history, y_target\n",
    "\n",
    "\n",
    "def adjust_learning_rate(net: DaRnnNet, n_iter: int):\n",
    "    # TODO: Where did this Learning Rate adjustment schedule come from?\n",
    "    # Should be modified to use Cosine Annealing with warm restarts https://www.jeremyjordan.me/nn-learning-rate/\n",
    "    if n_iter % 10000 == 0 and n_iter > 0:\n",
    "        for enc_params, dec_params in zip(net.enc_opt.param_groups, net.dec_opt.param_groups):\n",
    "            enc_params['lr'] = enc_params['lr'] * 0.9\n",
    "            dec_params['lr'] = dec_params['lr'] * 0.9\n",
    "\n",
    "\n",
    "def train_iteration(t_net: DaRnnNet, loss_func: typing.Callable, X, y_history, y_target):\n",
    "    t_net.enc_opt.zero_grad()\n",
    "    t_net.dec_opt.zero_grad()\n",
    "\n",
    "    input_weighted, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "    y_pred = t_net.decoder(input_encoded, numpy_to_tvar(y_history))\n",
    "\n",
    "    y_true = numpy_to_tvar(y_target)\n",
    "    loss = loss_func(y_pred, y_true)\n",
    "    loss.backward()\n",
    "\n",
    "    t_net.enc_opt.step()\n",
    "    t_net.dec_opt.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def predict(t_net: DaRnnNet, t_dat: TrainData, train_size: int, batch_size: int, T: int, on_train=False):\n",
    "    out_size = t_dat.targs.shape[1]\n",
    "    if on_train:\n",
    "        y_pred = np.zeros((train_size - T + 1, out_size))\n",
    "    else:\n",
    "        y_pred = np.zeros((t_dat.feats.shape[0] - train_size, out_size))\n",
    "\n",
    "    for y_i in range(0, len(y_pred), batch_size):\n",
    "        y_slc = slice(y_i, y_i + batch_size)\n",
    "        batch_idx = range(len(y_pred))[y_slc]\n",
    "        b_len = len(batch_idx)\n",
    "        X = np.zeros((b_len, T - 1, t_dat.feats.shape[1]))\n",
    "        y_history = np.zeros((b_len, T - 1, t_dat.targs.shape[1]))\n",
    "\n",
    "        for b_i, b_idx in enumerate(batch_idx):\n",
    "            if on_train:\n",
    "                idx = range(b_idx, b_idx + T - 1)\n",
    "            else:\n",
    "                idx = range(b_idx + train_size - T, b_idx + train_size - 1)\n",
    "\n",
    "            X[b_i, :, :] = t_dat.feats[idx, :]\n",
    "            y_history[b_i, :] = t_dat.targs[idx]\n",
    "\n",
    "        y_history = numpy_to_tvar(y_history)\n",
    "        _, input_encoded = t_net.encoder(numpy_to_tvar(X))\n",
    "        y_pred[y_slc] = t_net.decoder(input_encoded, y_history).cpu().data.numpy()\n",
    "\n",
    "    return y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-6594923f937a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdebug\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#TODO 1.读取数据集，如果是debug模式就读取前面100行 否则读取全部\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mraw_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"澳大利亚电力负荷与价格预测数据.xlsx\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mdebug\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "save_plots = True\n",
    "debug = True\n",
    "#TODO 1.读取数据集，如果是debug模式就读取前面100行 否则读取全部\n",
    "raw_data = pd.read_excel(os.path.join(\"data\", \"澳大利亚电力负荷与价格预测数据.xlsx\"), nrows=1000 if debug else None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logger.info(f\"Shape of data: {raw_data.shape}.\\nMissing in data: {raw_data.isnull().sum().sum()}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data_copy = raw_data.copy()\n",
    "raw_data_copy.columns = ['date','hour','f1','f2','f3','f4','f5','target']\n",
    "raw_data_copy = raw_data_copy[['f1','f2','f3','f4','f5','target']]\n",
    "targ_cols = (\"target\",) #NDX是我们需要预测的值\n",
    "data, scaler = preprocess_data(raw_data_copy, targ_cols)\n",
    "da_rnn_kwargs = {\"batch_size\": 128, \"T\": 10}\n",
    "config, model = da_rnn(data, n_targs=len(targ_cols), learning_rate=.001, **da_rnn_kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iter_loss, epoch_loss = train(model, data, config, n_epochs=30, save_plots=save_plots)\n",
    "del raw_data,raw_data_copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_y_pred = predict(model, data, config.train_size, config.batch_size, config.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}