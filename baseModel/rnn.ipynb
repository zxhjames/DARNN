{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import typing\r\n",
    "save_plot = True\r\n",
    "debug = True\r\n",
    "data_dir = '../data/dianli.xlsx'\r\n",
    "data = pd.read_excel(data_dir,nrows=1000 if debug else None)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "raw_data_copy = data.copy()\r\n",
    "raw_data_copy.columns = ['date', 'hour',\r\n",
    "                         'f1', 'f2', 'f3', 'f4', 'f5', 'target']\r\n",
    "raw_data_copy = raw_data_copy[['f1', 'f2', 'f3', 'f4', 'f5', 'target']]\r\n",
    "targ_cols = (\"target\",)  # NDX是我们需要预测的值"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.9</td>\n",
       "      <td>21.65</td>\n",
       "      <td>22.4</td>\n",
       "      <td>87.5</td>\n",
       "      <td>19.67</td>\n",
       "      <td>8013.27833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1     f2    f3    f4     f5      target\n",
       "0  23.9  21.65  22.4  87.5  19.67  8013.27833"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 数据预处理\r\n",
    "class TrainConfig(typing.NamedTuple):\r\n",
    "    T: int\r\n",
    "    train_size: int\r\n",
    "    batch_size: int\r\n",
    "    loss_func: typing.Callable\r\n",
    "\r\n",
    "\r\n",
    "class TrainData(typing.NamedTuple):\r\n",
    "    feats: np.ndarray\r\n",
    "    targs: np.ndarray\r\n",
    "    \r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "def read2Dataloader(data,col_names):\r\n",
    "    scaler = StandardScaler().fit(data)\r\n",
    "    proc_dat = scaler.transform(data)\r\n",
    "    mask = np.ones(proc_dat.shape[1],dtype=bool)\r\n",
    "    dat_cols = list(data.columns)\r\n",
    "    for col_name in col_names:\r\n",
    "        mask[dat_cols.index(col_name)] = False\r\n",
    "    feats = proc_dat[:,mask]\r\n",
    "    targs = proc_dat[:,~mask]\r\n",
    "    return TrainData(feats,targs)\r\n",
    "# 返回特征\r\n",
    "trainData = read2Dataloader(raw_data_copy,targ_cols)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "'''\r\n",
    "基本网络类型\r\n",
    "'''\r\n",
    "import torch.nn as nn\r\n",
    "import json\r\n",
    "from torch import optim\r\n",
    "import collections\r\n",
    "import typing\r\n",
    "\r\n",
    "'''\r\n",
    "初始化简单的lstm网络\r\n",
    "'''\r\n",
    "RnnNet = collections.namedtuple(\"RnnNet\",[\"rnn\",\"rnn_optimizer\"])\r\n",
    "class Lstm(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, input_size, hidden_size, num_layers , output_size , dropout, batch_first):\r\n",
    "        super(Lstm, self).__init__()\r\n",
    "        # lstm的输入 #batch,seq_len, input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.input_size = input_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.output_size = output_size\r\n",
    "        self.dropout = dropout\r\n",
    "        self.batch_first = batch_first\r\n",
    "\r\n",
    "        self.rnn = nn.GRU(input_size=self.input_size, \r\n",
    "                           hidden_size=self.hidden_size, \r\n",
    "                           num_layers=self.num_layers, \r\n",
    "                           batch_first=self.batch_first, \r\n",
    "                           dropout=self.dropout)\r\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out, (hidden, cell) = self.rnn(x)  # x.shape : batch, seq_len, hidden_size , hn.shape and cn.shape : num_layes * direction_numbers, batch, hidden_size\r\n",
    "        # a, b, c = hidden.shape\r\n",
    "        # out = self.linear(hidden.reshape(a * b, c))\r\n",
    "        out = self.linear(hidden)\r\n",
    "        return out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def rnn(train_data: TrainData, n_targs: int, hidden_size:int, T:int, learning_rate=0.001, batch_size=128):\r\n",
    "\r\n",
    "    # 定义配置器 T=>滑窗长度 截取前70%的数据作为训练集\r\n",
    "    train_cfg = TrainConfig(\r\n",
    "        T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\r\n",
    "    print('train size: ',train_cfg.train_size)\r\n",
    "\r\n",
    "\r\n",
    "    input_size = train_data.feats.shape[1] \r\n",
    "    print('input size: ',input_size +1)\r\n",
    "    # 初始化网络结构\r\n",
    "    rnn_args = {\r\n",
    "        \"input_size\" :input_size + 1,\r\n",
    "        \"hidden_size\" : hidden_size,\r\n",
    "        \"num_layers\" : 1,\r\n",
    "        \"output_size\" : 1,\r\n",
    "        \"dropout\" : 0,\r\n",
    "        \"batch_first\":True\r\n",
    "    }\r\n",
    "    print (\"run args: \", rnn_args)\r\n",
    "    rnn  = Lstm(**rnn_args)\r\n",
    "    with open( ('../data/lstm.json'),\"w\") as fi: \r\n",
    "        json.dump(rnn_args,fi,indent=4)\r\n",
    "\r\n",
    "    rnn_optimizer = optim.Adam(\r\n",
    "        params=rnn.parameters(),\r\n",
    "        lr=learning_rate\r\n",
    "    )\r\n",
    "    # 返回的网络结构\r\n",
    "    rnn_net = RnnNet(\r\n",
    "        rnn,rnn_optimizer\r\n",
    "    )\r\n",
    "    return train_cfg, rnn_net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# 初始化模型参数\r\n",
    "init_args = {\"batch_size\": 128, \"T\": 10}\r\n",
    "rnn_kwargs = init_args\r\n",
    "config, model = rnn(trainData, n_targs=len(targ_cols),hidden_size=64,T=10,learning_rate=.001,batch_size=128)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train size:  700\n",
      "input size:  6\n",
      "run args:  {'input_size': 6, 'hidden_size': 64, 'num_layers': 1, 'output_size': 1, 'dropout': 0, 'batch_first': True}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# 平滑处理序列 开始训练\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "def PrepareData(batch_idx,t_cfg,train_data):\r\n",
    "    feats = np.zeros((len(batch_idx), t_cfg.T - 1 , train_data.feats.shape[1]))\r\n",
    "    y_history = np.zeros((len(batch_idx) ,t_cfg.T - 1 , train_data.targs.shape[1]))\r\n",
    "    y_target = train_data.targs[batch_idx + t_cfg.T]\r\n",
    "    # 获取采样的batch_id的下标和值\r\n",
    "    # 获取特征和标签的相应下标值\r\n",
    "    for b_i, b_idx in enumerate(batch_idx):\r\n",
    "        b_slc = slice(b_idx, b_idx + t_cfg.T-1)\r\n",
    "        #print('b_i',b_i,'b_slc',b_slc)\r\n",
    "        feats[b_i, :, :] = train_data.feats[b_slc, :]\r\n",
    "        #print(y_history[b_i : ].shape,train_data.targs[b_slc].shape)\r\n",
    "        y_history[b_i :] = train_data.targs[b_slc]\r\n",
    "\r\n",
    "    return feats, y_history, y_target\r\n",
    "\r\n",
    "\r\n",
    "def rnn_train_iteration(t_net: RnnNet, loss_func: typing.Callable, X, y_history, y_target):\r\n",
    "    input_data = np.append(X, y_history, axis=2)\r\n",
    "    # print(input_data.shape)\r\n",
    "    data1 = torch.from_numpy(input_data).to(torch.float32)\r\n",
    "    pred = t_net.rnn(Variable(data1))\r\n",
    "    pred = pred[0, :, :]\r\n",
    "    label = torch.from_numpy(y_target).to(torch.float32).unsqueeze(1)\r\n",
    "    loss = loss_func(pred, label)\r\n",
    "    t_net.rnn_optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    t_net.rnn_optimizer.step()\r\n",
    "    print('loss计算完成')\r\n",
    "    return loss.item()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# 这一步用来进一步将数据处理成模型能够输入的形式\r\n",
    "def ProcessData(feats,y_history,y_target):\r\n",
    "    '''\r\n",
    "    input_data加工成 128,10,6的形状\r\n",
    "    Label加工为 128的形状\r\n",
    "    '''\r\n",
    "    print('加工数据u ')\r\n",
    "    print('feats',feats.shape)\r\n",
    "    print('y_history',y_history.shape)\r\n",
    "    print('y_target',y_target.shape)\r\n",
    "    label = torch.from_numpy(y_target).squeeze(1)\r\n",
    "    data = torch.cat([torch.from_numpy(feats),torch.from_numpy(y_history)],2)\r\n",
    "    return data,label\r\n",
    "\r\n",
    "def train_rnn(net: RnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs, save_plots=False):\r\n",
    "    # 完成所有数据训练的迭代次数\r\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\r\n",
    "    print('iter_per_epoch: ',t_cfg.train_size,t_cfg.batch_size,iter_per_epoch)\r\n",
    "    # 存储损失值列表\r\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\r\n",
    "    print('iter_losses: ',iter_losses.shape)\r\n",
    "    # 存储每次epoch的损失值\r\n",
    "    epoch_losses = np.zeros(n_epochs)\r\n",
    "   \r\n",
    "    n_iter = 0\r\n",
    "    print('一共', n_epochs, '次迭代')\r\n",
    "\r\n",
    "    for e_i in range(n_epochs):\r\n",
    "        print('现在是第 ', e_i, '轮迭代')\r\n",
    "        # 随机生成\r\n",
    "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\r\n",
    "\r\n",
    "        # 循环迭代 每次迭代的步长为batch_size，每次选择batch_size大小的数据进行预测\r\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\r\n",
    "            # 随机选择索引列\r\n",
    "            batch_idx = perm_idx[t_i:(t_i + t_cfg.batch_size)]\r\n",
    "            # TODO 闭包函数，返回处理好的特征，历史数据，预测目标值\r\n",
    "            feats,y_history,y_target = PrepareData(batch_idx,t_cfg,train_data)\r\n",
    "            # data,label = ProcessData(feats,y_history,y_target)\r\n",
    "            # 使用新的训练函数\r\n",
    "            loss = rnn_train_iteration(net, t_cfg.loss_func,\r\n",
    "                                       feats, y_history, y_target)\r\n",
    "            print('loss' , loss)\r\n",
    "\r\n",
    "        epoch_losses[e_i] = np.mean(\r\n",
    "            iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\r\n",
    "\r\n",
    "        print('epoch_loss',epoch_losses)\r\n",
    "    print('训练结束')\r\n",
    "    return iter_losses, epoch_losses\r\n",
    "\r\n",
    "print('model params: ',model.rnn)\r\n",
    "iter_loss, epoch_loss = train_rnn(\r\n",
    "            model, trainData, config, n_epochs=30, save_plots=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model params:  Lstm(\n",
      "  (rnn): GRU(6, 64, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "iter_per_epoch:  700 128 6\n",
      "iter_losses:  (180,)\n",
      "一共 30 次迭代\n",
      "现在是第  0 轮迭代\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-faf860f02ca3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model params: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m iter_loss, epoch_loss = train_rnn(\n\u001b[0m\u001b[0;32m     54\u001b[0m             model, trainData, config, n_epochs=30, save_plots=True)\n",
      "\u001b[1;32m<ipython-input-44-faf860f02ca3>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[1;34m(net, train_data, t_cfg, n_epochs, save_plots)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m# data,label = ProcessData(feats,y_history,y_target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# 使用新的训练函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             loss = rnn_train_iteration(net, t_cfg.loss_func,\n\u001b[0m\u001b[0;32m     42\u001b[0m                                        feats, y_history, y_target)\n\u001b[0;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-3c8f0697e1ba>\u001b[0m in \u001b[0;36mrnn_train_iteration\u001b[1;34m(t_net, loss_func, X, y_history, y_target)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# print(input_data.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-49cdfb6447c2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# x.shape : batch, seq_len, hidden_size , hn.shape and cn.shape : num_layes * direction_numbers, batch, hidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# a, b, c = hidden.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# out = self.linear(hidden.reshape(a * b, c))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}