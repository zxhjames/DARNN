{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "save_plot = True\r\n",
    "debug = True\r\n",
    "data_dir = '../data/dianli.xlsx'\r\n",
    "data = pd.read_excel(data_dir,nrows=1000 if debug else None)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "raw_data_copy = data.copy()\r\n",
    "raw_data_copy.columns = ['date', 'hour',\r\n",
    "                         'f1', 'f2', 'f3', 'f4', 'f5', 'target']\r\n",
    "raw_data_copy = raw_data_copy[['f1', 'f2', 'f3', 'f4', 'f5', 'target']]\r\n",
    "targ_cols = (\"target\",)  # NDX是我们需要预测的值"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# 数据预处理\r\n",
    "class TrainConfig(typing.NamedTuple):\r\n",
    "    T: int\r\n",
    "    train_size: int\r\n",
    "    batch_size: int\r\n",
    "    loss_func: typing.Callable\r\n",
    "\r\n",
    "\r\n",
    "class TrainData(typing.NamedTuple):\r\n",
    "    feats: np.ndarray\r\n",
    "    targs: np.ndarray\r\n",
    "    \r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "def read2Dataloader(data,T,batchSize):\r\n",
    "    l = len(data.columns) - 1\r\n",
    "    scale = StandardScaler().fit(data)\r\n",
    "    df = (scale.transform(data))\r\n",
    "    Y = df[:,l:]\r\n",
    "    X = df[:,0:l]\r\n",
    "    return TrainData(X,Y)\r\n",
    "# 返回特征\r\n",
    "trainData = read2Dataloader(raw_data_copy,10,128)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "trainData.targs.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "'''\r\n",
    "基本网络类型\r\n",
    "'''\r\n",
    "import torch.nn as nn\r\n",
    "import json\r\n",
    "from torch import optim\r\n",
    "import collections\r\n",
    "import typing\r\n",
    "\r\n",
    "'''\r\n",
    "初始化简单的lstm网络\r\n",
    "'''\r\n",
    "RnnNet = collections.namedtuple(\"RnnNet\",[\"rnn\",\"rnn_optimizer\"])\r\n",
    "class Lstm(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, input_size, hidden_size, num_layers , output_size , dropout, batch_first):\r\n",
    "        super(Lstm, self).__init__()\r\n",
    "        # lstm的输入 #batch,seq_len, input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.input_size = input_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.output_size = output_size\r\n",
    "        self.dropout = dropout\r\n",
    "        self.batch_first = batch_first\r\n",
    "\r\n",
    "        self.rnn = nn.GRU(input_size=self.input_size, \r\n",
    "                           hidden_size=self.hidden_size, \r\n",
    "                           num_layers=self.num_layers, \r\n",
    "                           batch_first=self.batch_first, \r\n",
    "                           dropout=self.dropout)\r\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out, (hidden, cell) = self.rnn(x)  # x.shape : batch, seq_len, hidden_size , hn.shape and cn.shape : num_layes * direction_numbers, batch, hidden_size\r\n",
    "        # a, b, c = hidden.shape\r\n",
    "        # out = self.linear(hidden.reshape(a * b, c))\r\n",
    "        out = self.linear(hidden)\r\n",
    "        return out\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def rnn(train_data: TrainData, n_targs: int, hidden_size:int, T:int, learning_rate=0.001, batch_size=128):\r\n",
    "\r\n",
    "    # 定义配置器 T=>滑窗长度 截取前70%的数据作为训练集\r\n",
    "    train_cfg = TrainConfig(\r\n",
    "        T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\r\n",
    "    print('train size: ',train_cfg.train_size)\r\n",
    "    input_size = train_data.feats.shape[1] \r\n",
    "    print('加1是为了增加历史列 input size: ',input_size)\r\n",
    "    # 初始化网络结构\r\n",
    "    rnn_args = {\r\n",
    "        \"input_size\" :input_size + 1,\r\n",
    "        \"hidden_size\" : hidden_size,\r\n",
    "        \"num_layers\" : 1,\r\n",
    "        \"output_size\" : 1,\r\n",
    "        \"dropout\" : 0,\r\n",
    "        \"batch_first\":True\r\n",
    "    }\r\n",
    "    print (\"run args: \", rnn_args)\r\n",
    "    rnn  = Lstm(**rnn_args)\r\n",
    "    with open( ('../data/lstm.json'),\"w\") as fi: \r\n",
    "        json.dump(rnn_args,fi,indent=4)\r\n",
    "\r\n",
    "    rnn_optimizer = optim.Adam(\r\n",
    "        params=rnn.parameters(),\r\n",
    "        lr=learning_rate\r\n",
    "    )\r\n",
    "    # 返回的网络结构\r\n",
    "    rnn_net = RnnNet(\r\n",
    "        rnn,rnn_optimizer\r\n",
    "    )\r\n",
    "    return train_cfg, rnn_net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# 初始化模型参数\r\n",
    "init_args = {\"batch_size\": 128, \"T\": 10}\r\n",
    "rnn_kwargs = init_args\r\n",
    "config, model = rnn(data, n_targs=len(targ_cols),hidden_size=64,T=10,learning_rate=.001,batch_size=128)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train size:  700\n",
      "加1是为了增加历史列 input size:  5\n",
      "run args:  {'input_size': 6, 'hidden_size': 64, 'num_layers': 1, 'output_size': 1, 'dropout': 0, 'batch_first': True}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# 平滑处理序列 开始训练\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "def PrepareData(batch_idx,t_cfg,train_data):\r\n",
    "    feats = np.zeros((len(batch_idx), t_cfg.T - 1 , train_data.feats.shape[1]))\r\n",
    "    y_history = np.zeros((len(batch_idx) ,t_cfg.T - 1 , train_data.targs.shape[1]))\r\n",
    "    y_target = train_data.targs[batch_idx + t_cfg.T]\r\n",
    "\r\n",
    "    print('y history shape' , y_history.shape)\r\n",
    "    print('train data targs  shape' , train_data.targs.shape)\r\n",
    "    # 获取采样的batch_id的下标和值\r\n",
    "    # 获取特征和标签的相应下标值\r\n",
    "    for b_i, b_idx in enumerate(batch_idx):\r\n",
    "        b_slc = slice(b_idx, b_idx + t_cfg.T-1)\r\n",
    "        #print('b_i',b_i,'b_slc',b_slc)\r\n",
    "        feats[b_i, :, :] = train_data.feats[b_slc, :]\r\n",
    "        #print(y_history[b_i : ].shape,train_data.targs[b_slc].shape)\r\n",
    "        y_history[b_i :] = train_data.targs[b_slc]\r\n",
    "\r\n",
    "    return feats, y_history, y_target\r\n",
    "\r\n",
    "\r\n",
    "def train_iteration(t_net: RnnNet, loss_func: typing.Callable, X, y_history, y_target):\r\n",
    "    input_data = np.append(X,y_history,axis=2)\r\n",
    "    print(input_data.shape)\r\n",
    "    data1 = torch.from_numpy(input_data).to(torch.float32)\r\n",
    "    print('data1 shape',data1.shape)\r\n",
    "    pred = t_net.rnn(Variable(data1))\r\n",
    "    pred = pred[0, :, :]\r\n",
    "    label = torch.from_numpy(y_target).to(torch.float32).unsqueeze(1)\r\n",
    "    loss = loss_func(pred, label)\r\n",
    "    t_net.rnn_optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    t_net.rnn_optimizer.step()\r\n",
    "    return loss.item()\r\n",
    "\r\n",
    "\r\n",
    "def train_rnn(net: RnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs, save_plots=False):\r\n",
    "    # 完成所有数据训练的迭代次数\r\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\r\n",
    "    print('iter_per_epoch: ',t_cfg.train_size,t_cfg.batch_size,iter_per_epoch)\r\n",
    "    # 存储损失值列表\r\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\r\n",
    "    print('iter_losses: ',iter_losses.shape)\r\n",
    "    # 存储每次epoch的损失值\r\n",
    "    epoch_losses = np.zeros(n_epochs)\r\n",
    "   \r\n",
    "    n_iter = 0\r\n",
    "    print('一共', n_epochs, '次迭代')\r\n",
    "\r\n",
    "    for e_i in range(n_epochs):\r\n",
    "        print('现在是第 ', e_i, '轮迭代')\r\n",
    "        # 随机生成\r\n",
    "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\r\n",
    "\r\n",
    "        # 循环迭代 每次迭代的步长为batch_size，每次选择batch_size大小的数据进行预测\r\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\r\n",
    "            # 随机选择索引列\r\n",
    "            batch_idx = perm_idx[t_i:(t_i + t_cfg.batch_size)]\r\n",
    "\r\n",
    "\r\n",
    "            # TODO 闭包函数，返回处理好的特征，历史数据，预测目标值\r\n",
    "           \r\n",
    "\r\n",
    "            feats,y_history,y_target = PrepareData(batch_idx,t_cfg,train_data)\r\n",
    "            print('feats',feats.shape)\r\n",
    "            print('y_history',y_history.shape)\r\n",
    "            print('y_target',y_target.shape)\r\n",
    "            return 2,2\r\n",
    "            loss = train_iteration(net, t_cfg.loss_func,\r\n",
    "                                   feats, y_history, y_target)\r\n",
    "            print('loss' , loss)\r\n",
    "\r\n",
    "        epoch_losses[e_i] = np.mean(\r\n",
    "            iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\r\n",
    "\r\n",
    "        print('epoch_loss',epoch_losses)\r\n",
    "    print('训练结束')\r\n",
    "    return iter_losses, epoch_losses\r\n",
    "\r\n",
    "iter_loss, epoch_loss = train_rnn(\r\n",
    "            model, data, config, n_epochs=30, save_plots=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iter_per_epoch:  700 128 6\n",
      "iter_losses:  (180,)\n",
      "一共 30 次迭代\n",
      "现在是第  0 轮迭代\n",
      "y history shape (128, 9, 1)\n",
      "train data targs  shape (1000, 1)\n",
      "b_i 0 b_slc slice(409, 418, None)\n",
      "(128, 9, 1) (9, 1)\n",
      "b_i 1 b_slc slice(288, 297, None)\n",
      "(127, 9, 1) (9, 1)\n",
      "b_i 2 b_slc slice(637, 646, None)\n",
      "(126, 9, 1) (9, 1)\n",
      "b_i 3 b_slc slice(552, 561, None)\n",
      "(125, 9, 1) (9, 1)\n",
      "b_i 4 b_slc slice(50, 59, None)\n",
      "(124, 9, 1) (9, 1)\n",
      "b_i 5 b_slc slice(60, 69, None)\n",
      "(123, 9, 1) (9, 1)\n",
      "b_i 6 b_slc slice(271, 280, None)\n",
      "(122, 9, 1) (9, 1)\n",
      "b_i 7 b_slc slice(138, 147, None)\n",
      "(121, 9, 1) (9, 1)\n",
      "b_i 8 b_slc slice(105, 114, None)\n",
      "(120, 9, 1) (9, 1)\n",
      "b_i 9 b_slc slice(137, 146, None)\n",
      "(119, 9, 1) (9, 1)\n",
      "b_i 10 b_slc slice(287, 296, None)\n",
      "(118, 9, 1) (9, 1)\n",
      "b_i 11 b_slc slice(315, 324, None)\n",
      "(117, 9, 1) (9, 1)\n",
      "b_i 12 b_slc slice(107, 116, None)\n",
      "(116, 9, 1) (9, 1)\n",
      "b_i 13 b_slc slice(669, 678, None)\n",
      "(115, 9, 1) (9, 1)\n",
      "b_i 14 b_slc slice(111, 120, None)\n",
      "(114, 9, 1) (9, 1)\n",
      "b_i 15 b_slc slice(396, 405, None)\n",
      "(113, 9, 1) (9, 1)\n",
      "b_i 16 b_slc slice(163, 172, None)\n",
      "(112, 9, 1) (9, 1)\n",
      "b_i 17 b_slc slice(47, 56, None)\n",
      "(111, 9, 1) (9, 1)\n",
      "b_i 18 b_slc slice(590, 599, None)\n",
      "(110, 9, 1) (9, 1)\n",
      "b_i 19 b_slc slice(147, 156, None)\n",
      "(109, 9, 1) (9, 1)\n",
      "b_i 20 b_slc slice(148, 157, None)\n",
      "(108, 9, 1) (9, 1)\n",
      "b_i 21 b_slc slice(369, 378, None)\n",
      "(107, 9, 1) (9, 1)\n",
      "b_i 22 b_slc slice(1, 10, None)\n",
      "(106, 9, 1) (9, 1)\n",
      "b_i 23 b_slc slice(135, 144, None)\n",
      "(105, 9, 1) (9, 1)\n",
      "b_i 24 b_slc slice(421, 430, None)\n",
      "(104, 9, 1) (9, 1)\n",
      "b_i 25 b_slc slice(670, 679, None)\n",
      "(103, 9, 1) (9, 1)\n",
      "b_i 26 b_slc slice(242, 251, None)\n",
      "(102, 9, 1) (9, 1)\n",
      "b_i 27 b_slc slice(422, 431, None)\n",
      "(101, 9, 1) (9, 1)\n",
      "b_i 28 b_slc slice(154, 163, None)\n",
      "(100, 9, 1) (9, 1)\n",
      "b_i 29 b_slc slice(545, 554, None)\n",
      "(99, 9, 1) (9, 1)\n",
      "b_i 30 b_slc slice(267, 276, None)\n",
      "(98, 9, 1) (9, 1)\n",
      "b_i 31 b_slc slice(428, 437, None)\n",
      "(97, 9, 1) (9, 1)\n",
      "b_i 32 b_slc slice(340, 349, None)\n",
      "(96, 9, 1) (9, 1)\n",
      "b_i 33 b_slc slice(166, 175, None)\n",
      "(95, 9, 1) (9, 1)\n",
      "b_i 34 b_slc slice(162, 171, None)\n",
      "(94, 9, 1) (9, 1)\n",
      "b_i 35 b_slc slice(54, 63, None)\n",
      "(93, 9, 1) (9, 1)\n",
      "b_i 36 b_slc slice(77, 86, None)\n",
      "(92, 9, 1) (9, 1)\n",
      "b_i 37 b_slc slice(304, 313, None)\n",
      "(91, 9, 1) (9, 1)\n",
      "b_i 38 b_slc slice(688, 697, None)\n",
      "(90, 9, 1) (9, 1)\n",
      "b_i 39 b_slc slice(562, 571, None)\n",
      "(89, 9, 1) (9, 1)\n",
      "b_i 40 b_slc slice(277, 286, None)\n",
      "(88, 9, 1) (9, 1)\n",
      "b_i 41 b_slc slice(575, 584, None)\n",
      "(87, 9, 1) (9, 1)\n",
      "b_i 42 b_slc slice(88, 97, None)\n",
      "(86, 9, 1) (9, 1)\n",
      "b_i 43 b_slc slice(265, 274, None)\n",
      "(85, 9, 1) (9, 1)\n",
      "b_i 44 b_slc slice(591, 600, None)\n",
      "(84, 9, 1) (9, 1)\n",
      "b_i 45 b_slc slice(539, 548, None)\n",
      "(83, 9, 1) (9, 1)\n",
      "b_i 46 b_slc slice(279, 288, None)\n",
      "(82, 9, 1) (9, 1)\n",
      "b_i 47 b_slc slice(164, 173, None)\n",
      "(81, 9, 1) (9, 1)\n",
      "b_i 48 b_slc slice(266, 275, None)\n",
      "(80, 9, 1) (9, 1)\n",
      "b_i 49 b_slc slice(678, 687, None)\n",
      "(79, 9, 1) (9, 1)\n",
      "b_i 50 b_slc slice(33, 42, None)\n",
      "(78, 9, 1) (9, 1)\n",
      "b_i 51 b_slc slice(534, 543, None)\n",
      "(77, 9, 1) (9, 1)\n",
      "b_i 52 b_slc slice(392, 401, None)\n",
      "(76, 9, 1) (9, 1)\n",
      "b_i 53 b_slc slice(405, 414, None)\n",
      "(75, 9, 1) (9, 1)\n",
      "b_i 54 b_slc slice(665, 674, None)\n",
      "(74, 9, 1) (9, 1)\n",
      "b_i 55 b_slc slice(22, 31, None)\n",
      "(73, 9, 1) (9, 1)\n",
      "b_i 56 b_slc slice(513, 522, None)\n",
      "(72, 9, 1) (9, 1)\n",
      "b_i 57 b_slc slice(115, 124, None)\n",
      "(71, 9, 1) (9, 1)\n",
      "b_i 58 b_slc slice(443, 452, None)\n",
      "(70, 9, 1) (9, 1)\n",
      "b_i 59 b_slc slice(110, 119, None)\n",
      "(69, 9, 1) (9, 1)\n",
      "b_i 60 b_slc slice(672, 681, None)\n",
      "(68, 9, 1) (9, 1)\n",
      "b_i 61 b_slc slice(478, 487, None)\n",
      "(67, 9, 1) (9, 1)\n",
      "b_i 62 b_slc slice(97, 106, None)\n",
      "(66, 9, 1) (9, 1)\n",
      "b_i 63 b_slc slice(84, 93, None)\n",
      "(65, 9, 1) (9, 1)\n",
      "b_i 64 b_slc slice(103, 112, None)\n",
      "(64, 9, 1) (9, 1)\n",
      "b_i 65 b_slc slice(629, 638, None)\n",
      "(63, 9, 1) (9, 1)\n",
      "b_i 66 b_slc slice(403, 412, None)\n",
      "(62, 9, 1) (9, 1)\n",
      "b_i 67 b_slc slice(542, 551, None)\n",
      "(61, 9, 1) (9, 1)\n",
      "b_i 68 b_slc slice(325, 334, None)\n",
      "(60, 9, 1) (9, 1)\n",
      "b_i 69 b_slc slice(686, 695, None)\n",
      "(59, 9, 1) (9, 1)\n",
      "b_i 70 b_slc slice(247, 256, None)\n",
      "(58, 9, 1) (9, 1)\n",
      "b_i 71 b_slc slice(213, 222, None)\n",
      "(57, 9, 1) (9, 1)\n",
      "b_i 72 b_slc slice(130, 139, None)\n",
      "(56, 9, 1) (9, 1)\n",
      "b_i 73 b_slc slice(294, 303, None)\n",
      "(55, 9, 1) (9, 1)\n",
      "b_i 74 b_slc slice(24, 33, None)\n",
      "(54, 9, 1) (9, 1)\n",
      "b_i 75 b_slc slice(15, 24, None)\n",
      "(53, 9, 1) (9, 1)\n",
      "b_i 76 b_slc slice(257, 266, None)\n",
      "(52, 9, 1) (9, 1)\n",
      "b_i 77 b_slc slice(73, 82, None)\n",
      "(51, 9, 1) (9, 1)\n",
      "b_i 78 b_slc slice(440, 449, None)\n",
      "(50, 9, 1) (9, 1)\n",
      "b_i 79 b_slc slice(388, 397, None)\n",
      "(49, 9, 1) (9, 1)\n",
      "b_i 80 b_slc slice(655, 664, None)\n",
      "(48, 9, 1) (9, 1)\n",
      "b_i 81 b_slc slice(337, 346, None)\n",
      "(47, 9, 1) (9, 1)\n",
      "b_i 82 b_slc slice(573, 582, None)\n",
      "(46, 9, 1) (9, 1)\n",
      "b_i 83 b_slc slice(232, 241, None)\n",
      "(45, 9, 1) (9, 1)\n",
      "b_i 84 b_slc slice(680, 689, None)\n",
      "(44, 9, 1) (9, 1)\n",
      "b_i 85 b_slc slice(358, 367, None)\n",
      "(43, 9, 1) (9, 1)\n",
      "b_i 86 b_slc slice(82, 91, None)\n",
      "(42, 9, 1) (9, 1)\n",
      "b_i 87 b_slc slice(262, 271, None)\n",
      "(41, 9, 1) (9, 1)\n",
      "b_i 88 b_slc slice(547, 556, None)\n",
      "(40, 9, 1) (9, 1)\n",
      "b_i 89 b_slc slice(306, 315, None)\n",
      "(39, 9, 1) (9, 1)\n",
      "b_i 90 b_slc slice(324, 333, None)\n",
      "(38, 9, 1) (9, 1)\n",
      "b_i 91 b_slc slice(40, 49, None)\n",
      "(37, 9, 1) (9, 1)\n",
      "b_i 92 b_slc slice(182, 191, None)\n",
      "(36, 9, 1) (9, 1)\n",
      "b_i 93 b_slc slice(461, 470, None)\n",
      "(35, 9, 1) (9, 1)\n",
      "b_i 94 b_slc slice(400, 409, None)\n",
      "(34, 9, 1) (9, 1)\n",
      "b_i 95 b_slc slice(10, 19, None)\n",
      "(33, 9, 1) (9, 1)\n",
      "b_i 96 b_slc slice(519, 528, None)\n",
      "(32, 9, 1) (9, 1)\n",
      "b_i 97 b_slc slice(318, 327, None)\n",
      "(31, 9, 1) (9, 1)\n",
      "b_i 98 b_slc slice(174, 183, None)\n",
      "(30, 9, 1) (9, 1)\n",
      "b_i 99 b_slc slice(351, 360, None)\n",
      "(29, 9, 1) (9, 1)\n",
      "b_i 100 b_slc slice(176, 185, None)\n",
      "(28, 9, 1) (9, 1)\n",
      "b_i 101 b_slc slice(568, 577, None)\n",
      "(27, 9, 1) (9, 1)\n",
      "b_i 102 b_slc slice(299, 308, None)\n",
      "(26, 9, 1) (9, 1)\n",
      "b_i 103 b_slc slice(613, 622, None)\n",
      "(25, 9, 1) (9, 1)\n",
      "b_i 104 b_slc slice(41, 50, None)\n",
      "(24, 9, 1) (9, 1)\n",
      "b_i 105 b_slc slice(522, 531, None)\n",
      "(23, 9, 1) (9, 1)\n",
      "b_i 106 b_slc slice(368, 377, None)\n",
      "(22, 9, 1) (9, 1)\n",
      "b_i 107 b_slc slice(134, 143, None)\n",
      "(21, 9, 1) (9, 1)\n",
      "b_i 108 b_slc slice(216, 225, None)\n",
      "(20, 9, 1) (9, 1)\n",
      "b_i 109 b_slc slice(143, 152, None)\n",
      "(19, 9, 1) (9, 1)\n",
      "b_i 110 b_slc slice(604, 613, None)\n",
      "(18, 9, 1) (9, 1)\n",
      "b_i 111 b_slc slice(329, 338, None)\n",
      "(17, 9, 1) (9, 1)\n",
      "b_i 112 b_slc slice(414, 423, None)\n",
      "(16, 9, 1) (9, 1)\n",
      "b_i 113 b_slc slice(663, 672, None)\n",
      "(15, 9, 1) (9, 1)\n",
      "b_i 114 b_slc slice(506, 515, None)\n",
      "(14, 9, 1) (9, 1)\n",
      "b_i 115 b_slc slice(215, 224, None)\n",
      "(13, 9, 1) (9, 1)\n",
      "b_i 116 b_slc slice(667, 676, None)\n",
      "(12, 9, 1) (9, 1)\n",
      "b_i 117 b_slc slice(261, 270, None)\n",
      "(11, 9, 1) (9, 1)\n",
      "b_i 118 b_slc slice(366, 375, None)\n",
      "(10, 9, 1) (9, 1)\n",
      "b_i 119 b_slc slice(319, 328, None)\n",
      "(9, 9, 1) (9, 1)\n",
      "b_i 120 b_slc slice(434, 443, None)\n",
      "(8, 9, 1) (9, 1)\n",
      "b_i 121 b_slc slice(131, 140, None)\n",
      "(7, 9, 1) (9, 1)\n",
      "b_i 122 b_slc slice(280, 289, None)\n",
      "(6, 9, 1) (9, 1)\n",
      "b_i 123 b_slc slice(581, 590, None)\n",
      "(5, 9, 1) (9, 1)\n",
      "b_i 124 b_slc slice(313, 322, None)\n",
      "(4, 9, 1) (9, 1)\n",
      "b_i 125 b_slc slice(622, 631, None)\n",
      "(3, 9, 1) (9, 1)\n",
      "b_i 126 b_slc slice(59, 68, None)\n",
      "(2, 9, 1) (9, 1)\n",
      "b_i 127 b_slc slice(472, 481, None)\n",
      "(1, 9, 1) (9, 1)\n",
      "feats (128, 9, 5)\n",
      "y_history (128, 9, 1)\n",
      "y_target (128, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}