{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import typing\r\n",
    "save_plot = True\r\n",
    "debug = True\r\n",
    "data_dir = '../data/dianli.xlsx'\r\n",
    "data = pd.read_excel(data_dir,nrows=1000 if debug else None)\r\n",
    "\r\n",
    "raw_data_copy = data.copy()\r\n",
    "raw_data_copy.columns = ['date', 'hour',\r\n",
    "                         'f1', 'f2', 'f3', 'f4', 'f5', 'target']\r\n",
    "raw_data_copy = raw_data_copy[['f1', 'f2', 'f3', 'f4', 'f5', 'target']]\r\n",
    "targ_cols = (\"target\",)  # NDX是我们需要预测的值\r\n",
    "\r\n",
    "# 数据预处理\r\n",
    "class TrainConfig(typing.NamedTuple):\r\n",
    "    T: int\r\n",
    "    train_size: int\r\n",
    "    batch_size: int\r\n",
    "    loss_func: typing.Callable\r\n",
    "\r\n",
    "\r\n",
    "class TrainData(typing.NamedTuple):\r\n",
    "    feats: np.ndarray\r\n",
    "    targs: np.ndarray\r\n",
    "    \r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "def read2Dataloader(data,col_names):\r\n",
    "    scaler = StandardScaler().fit(data)\r\n",
    "    proc_dat = scaler.transform(data)\r\n",
    "    mask = np.ones(proc_dat.shape[1],dtype=bool)\r\n",
    "    dat_cols = list(data.columns)\r\n",
    "    for col_name in col_names:\r\n",
    "        mask[dat_cols.index(col_name)] = False\r\n",
    "    feats = proc_dat[:,mask]\r\n",
    "    targs = proc_dat[:,~mask]\r\n",
    "    return TrainData(feats,targs)\r\n",
    "# 返回特征\r\n",
    "trainData = read2Dataloader(raw_data_copy,targ_cols)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "'''\r\n",
    "基本网络类型\r\n",
    "'''\r\n",
    "import torch.nn as nn\r\n",
    "import json\r\n",
    "from torch import optim\r\n",
    "import collections\r\n",
    "import typing\r\n",
    "import torch.nn.functional as F\r\n",
    "'''\r\n",
    "初始化简单的lstm网络\r\n",
    "'''\r\n",
    "RnnNet = collections.namedtuple(\"RnnNet\",[\"rnn\",\"rnn_optimizer\"])\r\n",
    "RnnAttNet = collections.namedtuple(\"RnnAttNet\",[\"rnn\",\"rnn_optimizer\"])\r\n",
    "\r\n",
    "class Lstm(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, input_size, hidden_size, num_layers , output_size , dropout, batch_first):\r\n",
    "        super(Lstm, self).__init__()\r\n",
    "        # lstm的输入 #batch,seq_len, input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.input_size = input_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.output_size = output_size\r\n",
    "        self.dropout = dropout\r\n",
    "        self.batch_first = batch_first\r\n",
    "\r\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\r\n",
    "                        num_layers=num_layers,batch_first=True)\r\n",
    "        # self.rnn = nn.LSTM(input_size=self.input_size, \r\n",
    "        #                    hidden_size=self.hidden_size, \r\n",
    "        #                    num_layers=self.num_layers, \r\n",
    "        #                    batch_first=self.batch_first, \r\n",
    "        #                    dropout=self.dropout)\r\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "       # print('x:shape',x.shape)\r\n",
    "        out, (hidden, cell) = self.rnn(x)  # x.shape : batch, seq_len, hidden_size , hn.shape and cn.shape : num_layes * direction_numbers, batch, hidden_size\r\n",
    "        # a, b, c = hidden.shape\r\n",
    "        # out = self.linear(hidden.reshape(a * b, c))\r\n",
    "       # print('out shape:',out.shape,'hidden shape:',hidden.shape,'cell shape:' ,cell.shape)\r\n",
    "        out = self.linear(hidden)\r\n",
    "        print(out.shape)\r\n",
    "\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.autograd import Variable\r\n",
    "from torch.nn import functional as tf\r\n",
    "class BiLSTM_Attention(nn.Module):\r\n",
    "    def __init__(self,input_size: int, hidden_size: int, T: int,bidirectional: True,num_classes: int):\r\n",
    "        super(BiLSTM_Attention, self).__init__()\r\n",
    "        self.input_size = input_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        \r\n",
    "        self.T = T\r\n",
    "        self.bidirectional = bidirectional\r\n",
    "        self.num_classes = num_classes\r\n",
    "        self.lstm = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\r\n",
    "                    num_layers=1,batch_first=True,bidirectional=self.bidirectional)\r\n",
    "        self.tanh1 = nn.Tanh()\r\n",
    "        self.w = nn.Parameter(torch.zeros(hidden_size * 2))\r\n",
    "        self.tanh2 = nn.Tanh()\r\n",
    "        self.fc1 = nn.Linear(hidden_size * 2,64)\r\n",
    "        self.out = nn.Linear(64, num_classes)\r\n",
    "\r\n",
    "    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\r\n",
    "    # def attention_net(self, lstm_output, final_state):\r\n",
    "    #     hidden = final_state.view(-1, self.hidden_size, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\r\n",
    "    #     attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\r\n",
    "    #     soft_attn_weights = F.softmax(attn_weights, 1)\r\n",
    "    #     # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\r\n",
    "    #     context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\r\n",
    "    #     return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\r\n",
    "\r\n",
    "    def forward(self, X):\r\n",
    "        # input = X.permute(1, 0, 2)\r\n",
    "        # print('shape input',input.shape)\r\n",
    "        # #input = X\r\n",
    "        # # input : [len_seq, batch_size, embedding_dim]\r\n",
    "        # hidden_state = Variable(torch.zeros(1, X.size(1), self.hidden_size)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\r\n",
    "        # cell_state = Variable(torch.zeros(1, X.size(1), self.hidden_size)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\r\n",
    "\r\n",
    "        # print('hidden shape',hidden_state.shape)\r\n",
    "\r\n",
    "        # # out, (hidden, cell) = self.lstm(X)\r\n",
    "        # # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\r\n",
    "        # output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\r\n",
    "\r\n",
    "        # output, (final_hidden_state, cell) = self.lstm(X) \r\n",
    "        # print(output.shape)\r\n",
    "\r\n",
    "        # output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\r\n",
    "        # attn_output, attention = self.attention_net(output, final_hidden_state)\r\n",
    "        # return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]\r\n",
    "        print(X.shape)\r\n",
    "        H,_ = self.lstm(X)\r\n",
    "        M = self.tanh1(H)\r\n",
    "        print(H.shape,M.shape,self.w.shape)\r\n",
    "        alpha = tf.softmax(torch.chain_matmul(M,self.w),dim=1).unsqueeze(-1)\r\n",
    "        out = H * alpha\r\n",
    "        out = torch.sum(out,1)\r\n",
    "        out = F.relu(out)\r\n",
    "        out = self.fc1(out)\r\n",
    "        out = self.out(out)\r\n",
    "        return out\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def rnnatt(train_data: TrainData, n_targs: int, hidden_size:int, T:int, learning_rate=0.001, batch_size=128):\r\n",
    "    # 定义配置器 T=>滑窗长度 截取前70%的数据作为训练集\r\n",
    "    train_cfg = TrainConfig(\r\n",
    "        T, int(train_data.feats.shape[0] * 0.7), batch_size, nn.MSELoss())\r\n",
    "    print('train size: ',train_cfg.train_size)\r\n",
    "\r\n",
    "\r\n",
    "    input_size = train_data.feats.shape[1] \r\n",
    "    print('input size: ',input_size +1)\r\n",
    "    # 初始化网络结构\r\n",
    "    # input_size: int, hidden_size: int, T: int,bidirectional: False,num_classes: int\r\n",
    "    rnnatt_args = {\r\n",
    "        \"input_size\" :input_size + 1,\r\n",
    "        \"hidden_size\" : hidden_size,\r\n",
    "        'bidirectional' :  False,\r\n",
    "        \"T\" : 10,\r\n",
    "        \"num_classes\" : 1,\r\n",
    "    }\r\n",
    "    print (\"run args: \", rnnatt_args)\r\n",
    "    rnn  = BiLSTM_Attention(**rnnatt_args)\r\n",
    "    with open( ('../data/lstmatt.json'),\"w\") as fi: \r\n",
    "        json.dump(rnnatt_args,fi,indent=4)\r\n",
    "\r\n",
    "    rnn_optimizer = optim.Adam(\r\n",
    "        params=rnn.parameters(),\r\n",
    "        lr=learning_rate\r\n",
    "    )\r\n",
    "    # 返回的网络结构\r\n",
    "    rnnatt_net = RnnAttNet(\r\n",
    "        rnn,rnn_optimizer\r\n",
    "    )\r\n",
    "    return train_cfg, rnnatt_net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# 初始化模型参数\r\n",
    "init_args = {\"batch_size\": 128, \"T\": 10}\r\n",
    "\r\n",
    "rnn_kwargs = rnn_att_kwargs = init_args\r\n",
    "\r\n",
    "config, model = rnn(trainData, n_targs=len(targ_cols),hidden_size=64,T=10,learning_rate=.001,batch_size=128)\r\n",
    "\r\n",
    "config1, model1 = rnnatt(trainData, n_targs=len(targ_cols),hidden_size=128,T=10,learning_rate=.001,batch_size=128)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train size:  700\n",
      "input size:  6\n",
      "run args:  {'input_size': 6, 'hidden_size': 64, 'num_layers': 1, 'output_size': 1, 'dropout': 0, 'batch_first': True}\n",
      "train size:  700\n",
      "input size:  6\n",
      "run args:  {'input_size': 6, 'hidden_size': 128, 'bidirectional': False, 'T': 10, 'num_classes': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# 平滑处理序列 开始训练\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "def PrepareData(batch_idx,t_cfg,train_data):\r\n",
    "    feats = np.zeros((len(batch_idx), t_cfg.T - 1 , train_data.feats.shape[1]))\r\n",
    "    y_history = np.zeros((len(batch_idx) ,t_cfg.T - 1 , train_data.targs.shape[1]))\r\n",
    "    y_target = train_data.targs[batch_idx + t_cfg.T]\r\n",
    "    # 获取采样的batch_id的下标和值\r\n",
    "    # 获取特征和标签的相应下标值\r\n",
    "    for b_i, b_idx in enumerate(batch_idx):\r\n",
    "        b_slc = slice(b_idx, b_idx + t_cfg.T-1)\r\n",
    "        #print('b_i',b_i,'b_slc',b_slc)\r\n",
    "        feats[b_i, :, :] = train_data.feats[b_slc, :]\r\n",
    "        #print(y_history[b_i : ].shape,train_data.targs[b_slc].shape)\r\n",
    "        y_history[b_i :] = train_data.targs[b_slc]\r\n",
    "\r\n",
    "    return feats, y_history, y_target\r\n",
    "\r\n",
    "\r\n",
    "def rnn_train_iteration(t_net: RnnNet, loss_func: typing.Callable, X, y_history, y_target):\r\n",
    "    input_data = np.append(X, y_history, axis=2)\r\n",
    "    data1 = torch.from_numpy(input_data).to(torch.float32)\r\n",
    "    #print('input shape: ',data1.shape)\r\n",
    "    pred = t_net.rnn(Variable(data1))\r\n",
    "   # print('pred shape: ',pred.shape)\r\n",
    "    #pred = pred[0, :, :]\r\n",
    "    label = torch.from_numpy(y_target).to(torch.float32).unsqueeze(1)\r\n",
    "   # print('label shape: ',label.shape)\r\n",
    "    loss = loss_func(pred, label)\r\n",
    "    t_net.rnn_optimizer.zero_grad()\r\n",
    "    loss.backward()\r\n",
    "    t_net.rnn_optimizer.step()\r\n",
    "    return loss.item()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "'''\r\n",
    "utils\r\n",
    "'''\r\n",
    "\r\n",
    "import logging\r\n",
    "import os\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "\r\n",
    "def setup_log(tag='VOC_TOPICS'):\r\n",
    "    # create logger\r\n",
    "    logger = logging.getLogger(tag)\r\n",
    "    # logger.handlers = []\r\n",
    "    logger.propagate = False\r\n",
    "    logger.setLevel(logging.DEBUG)\r\n",
    "    # create console handler and set level to debug\r\n",
    "    ch = logging.StreamHandler()\r\n",
    "    ch.setLevel(logging.DEBUG)\r\n",
    "    # create formatter\r\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
    "    # add formatter to ch\r\n",
    "    ch.setFormatter(formatter)\r\n",
    "    # add ch to logger\r\n",
    "    # logger.handlers = []\r\n",
    "    logger.addHandler(ch)\r\n",
    "    return logger\r\n",
    "\r\n",
    "\r\n",
    "def save_or_show_plot(file_nm: str, save: bool):\r\n",
    "    if save:\r\n",
    "        plt.savefig(file_nm)\r\n",
    "    else:\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "\r\n",
    "def numpy_to_tvar(x):\r\n",
    "    return Variable(torch.from_numpy(x).type(torch.FloatTensor).to(device))\r\n",
    "\r\n",
    "\r\n",
    "from sklearn.utils.validation import check_consistent_length, check_array\r\n",
    "\r\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\r\n",
    "                                   sample_weight=None,\r\n",
    "                                   multioutput='uniform_average'):\r\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\r\n",
    "        y_true, y_pred, multioutput)\r\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\r\n",
    "    epsilon = np.finfo(np.float64).eps\r\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\r\n",
    "    output_errors = np.average(mape,\r\n",
    "                               weights=sample_weight, axis=0)\r\n",
    "    if isinstance(multioutput, str):\r\n",
    "        if multioutput == 'raw_values':\r\n",
    "            return output_errors\r\n",
    "        elif multioutput == 'uniform_average':\r\n",
    "            # pass None as weights to np.average: uniform mean\r\n",
    "            multioutput = None\r\n",
    "\r\n",
    "    return np.average(output_errors, weights=multioutput)\r\n",
    "\r\n",
    "def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\r\n",
    "    check_consistent_length(y_true, y_pred)\r\n",
    "    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\r\n",
    "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\r\n",
    "\r\n",
    "    if y_true.ndim == 1:\r\n",
    "        y_true = y_true.reshape((-1, 1))\r\n",
    "\r\n",
    "    if y_pred.ndim == 1:\r\n",
    "        y_pred = y_pred.reshape((-1, 1))\r\n",
    "\r\n",
    "    if y_true.shape[1] != y_pred.shape[1]:\r\n",
    "        raise ValueError(\"y_true and y_pred have different number of output \"\r\n",
    "                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\r\n",
    "\r\n",
    "    n_outputs = y_true.shape[1]\r\n",
    "    allowed_multioutput_str = ('raw_values', 'uniform_average',\r\n",
    "                               'variance_weighted')\r\n",
    "    if isinstance(multioutput, str):\r\n",
    "        if multioutput not in allowed_multioutput_str:\r\n",
    "            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\r\n",
    "                             \"You provided multioutput={!r}\".format(\r\n",
    "                                 allowed_multioutput_str,\r\n",
    "                                 multioutput))\r\n",
    "    elif multioutput is not None:\r\n",
    "        multioutput = check_array(multioutput, ensure_2d=False)\r\n",
    "        if n_outputs == 1:\r\n",
    "            raise ValueError(\"Custom weights are useful only in \"\r\n",
    "                             \"multi-output cases.\")\r\n",
    "        elif n_outputs != len(multioutput):\r\n",
    "            raise ValueError((\"There must be equally many custom weights \"\r\n",
    "                              \"(%d) as outputs (%d).\") %\r\n",
    "                             (len(multioutput), n_outputs))\r\n",
    "    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\r\n",
    "\r\n",
    "    return y_type, y_true, y_pred, multioutput\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "logger = setup_log()\r\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\r\n",
    "\r\n",
    "\r\n",
    "def adjust_learning_rate(net: RnnNet, n_iter: int):\r\n",
    "    # TODO: Where did this Learning Rate adjustment schedule come from?\r\n",
    "    # Should be modified to use Cosine Annealing with warm restarts https://www.jeremyjordan.me/nn-learning-rate/\r\n",
    "    if n_iter % 10000 == 0 and n_iter > 0:\r\n",
    "        for enc_params, dec_params in zip(net.enc_opt.param_groups, net.dec_opt.param_groups):\r\n",
    "            enc_params['lr'] = enc_params['lr'] * 0.9\r\n",
    "            dec_params['lr'] = dec_params['lr'] * 0.9\r\n",
    "\r\n",
    "'''\r\n",
    "预测代码\r\n",
    "'''\r\n",
    "def predict(t_net: RnnNet, t_dat: TrainData, train_size: int, batch_size: int, T: int, on_train=False):\r\n",
    "    # 设置输出的维度\r\n",
    "    out_size = t_dat.targs.shape[1]\r\n",
    "\r\n",
    "    if on_train:\r\n",
    "        y_pred = np.zeros((train_size - T + 1, out_size))\r\n",
    "    else:\r\n",
    "        # 如果是测试就设置输出的长度为测试数据长\r\n",
    "        y_pred = np.zeros((t_dat.feats.shape[0] - train_size, out_size))\r\n",
    "\r\n",
    "    '''\r\n",
    "    以每一次batch大小重新批量预测数据\r\n",
    "    '''\r\n",
    "    for y_i in range(0, len(y_pred), batch_size):\r\n",
    "        y_slc = slice(y_i, y_i + batch_size)\r\n",
    "        batch_idx = range(len(y_pred))[y_slc]\r\n",
    "        X = np.zeros((len(batch_idx), T - 1, t_dat.feats.shape[1] ))\r\n",
    "        y_history = np.zeros((len(batch_idx), T - 1, t_dat.targs.shape[1]))\r\n",
    "\r\n",
    "        for b_i, b_idx in enumerate(batch_idx):\r\n",
    "            if on_train:\r\n",
    "                idx = range(b_idx, b_idx + T - 1)\r\n",
    "            else:\r\n",
    "                idx = range(b_idx + train_size - T, b_idx + train_size - 1)\r\n",
    "\r\n",
    "            X[b_i, :, :] = t_dat.feats[idx, :]\r\n",
    "            y_history[b_i, :] = t_dat.targs[idx]\r\n",
    "\r\n",
    "        '''\r\n",
    "        在这里重新合并rnn数据格式\r\n",
    "        '''\r\n",
    "        input_data = torch.from_numpy(np.append(X, y_history, axis=2)).to(torch.float32)\r\n",
    "        y_pred[y_slc] = t_net.rnn(Variable(input_data)).detach().numpy()\r\n",
    "    return y_pred\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def train_rnn(net: RnnNet, train_data: TrainData, t_cfg: TrainConfig, n_epochs, save_plots=False):\r\n",
    "    # 完成所有数据训练的迭代次数\r\n",
    "    iter_per_epoch = int(np.ceil(t_cfg.train_size * 1. / t_cfg.batch_size))\r\n",
    "    print('iter_per_epoch: ',t_cfg.train_size,t_cfg.batch_size,iter_per_epoch)\r\n",
    "    # 存储损失值列表\r\n",
    "    iter_losses = np.zeros(n_epochs * iter_per_epoch)\r\n",
    "    print('iter_losses: ',iter_losses.shape)\r\n",
    "    # 存储每次epoch的损失值\r\n",
    "    epoch_losses = np.zeros(n_epochs)\r\n",
    "   \r\n",
    "    n_iter = 0\r\n",
    "\r\n",
    "    for e_i in range(n_epochs):\r\n",
    "        # 随机生成\r\n",
    "        perm_idx = np.random.permutation(t_cfg.train_size - t_cfg.T)\r\n",
    "\r\n",
    "        # 循环迭代 每次迭代的步长为batch_size，每次选择batch_size大小的数据进行预测\r\n",
    "        for t_i in range(0, t_cfg.train_size, t_cfg.batch_size):\r\n",
    "            # 随机选择索引列\r\n",
    "            batch_idx = perm_idx[t_i:(t_i + t_cfg.batch_size)]\r\n",
    "            # TODO 闭包函数，返回处理好的特征，历史数据，预测目标值\r\n",
    "            feats,y_history,y_target = PrepareData(batch_idx,t_cfg,train_data)\r\n",
    "            # data,label = ProcessData(feats,y_history,y_target)\r\n",
    "            # 使用新的训练函数\r\n",
    "            loss = rnn_train_iteration(net, t_cfg.loss_func,\r\n",
    "                                       feats, y_history, y_target)\r\n",
    "                                    \r\n",
    "            iter_losses[e_i * iter_per_epoch + t_i // t_cfg.batch_size] = loss\r\n",
    "            # if (j / t_cfg.batch_size) % 50 == 0:\r\n",
    "            #    self.logger.info(\"Epoch %d, Batch %d: loss = %3.3f.\", i, j / t_cfg.batch_size, loss)\r\n",
    "            n_iter += 1\r\n",
    "            adjust_learning_rate(net, n_iter)\r\n",
    "            \r\n",
    "        epoch_losses[e_i] = np.mean(\r\n",
    "            iter_losses[range(e_i * iter_per_epoch, (e_i + 1) * iter_per_epoch)])\r\n",
    "        print ('epoch loss: ',epoch_loss[e_i])\r\n",
    "        if e_i % 10 == 0:\r\n",
    "            with torch.no_grad():\r\n",
    "                y_test_pred = predict(net, train_data,\r\n",
    "                                    t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\r\n",
    "                                    on_train=False)\r\n",
    "                # TODO: make this MSE and make it work for multiple inputs\r\n",
    "                val_loss = y_test_pred - train_data.targs[t_cfg.train_size:]\r\n",
    "                logger.info(\r\n",
    "                    f\"Epoch {e_i:d}, train loss: {epoch_losses[e_i]:3.3f}, val loss: {np.mean(np.abs(val_loss))}.\")\r\n",
    "                \r\n",
    "                y_train_pred = predict(net, train_data,\r\n",
    "                                    t_cfg.train_size, t_cfg.batch_size, t_cfg.T,\r\n",
    "                                    on_train=True)\r\n",
    "                plt.figure()\r\n",
    "                plt.plot(range(1, 1 + len(train_data.targs)), train_data.targs,\r\n",
    "                        label=\"True\")\r\n",
    "                plt.plot(range(t_cfg.T, len(y_train_pred) + t_cfg.T), y_train_pred,\r\n",
    "                        label='Predicted - Train')\r\n",
    "                plt.plot(range(t_cfg.T + len(y_train_pred), len(train_data.targs) + 1), y_test_pred,\r\n",
    "                        label='Predicted - Test')\r\n",
    "                plt.legend(loc='upper left')\r\n",
    "\r\n",
    "                # print(1,1 + len(train_data.targs))\r\n",
    "                # print(t_cfg.T,len(y_train_pred) + t_cfg.T)\r\n",
    "                # print(t_cfg.T + len(y_train_pred),len(train_data.targs) +1)\r\n",
    "\r\n",
    "\r\n",
    "                # todo 计算三者最后的MSE MAE MAPE\r\n",
    "                y_test_list = list(y_test_pred)\r\n",
    "                y_real = list(train_data.targs)[t_cfg.T + len(y_train_pred)-1 :len(train_data.targs) ]\r\n",
    "                # print(len(y_real),len(y_test_list))\r\n",
    "\r\n",
    "\r\n",
    "                print('rmse: ',np.sqrt(mean_squared_error(y_real,y_test_list)))\r\n",
    "                print('mae: ', mean_absolute_error(y_real, y_test_list))\r\n",
    "                print('mape: ', mean_absolute_percentage_error(y_real, y_test_list))\r\n",
    "                save_or_show_plot(f\"pred_rnn{e_i}.png\", save_plots)\r\n",
    "\r\n",
    "    print('epoch_loss',epoch_losses)\r\n",
    "    print('训练结束')\r\n",
    "    return iter_losses, epoch_losses\r\n",
    "\r\n",
    "print('model params: ',model.rnn)\r\n",
    "iter_loss, epoch_loss = train_rnn(\r\n",
    "            model1, trainData, config1, n_epochs=30, save_plots=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model params:  Lstm(\n",
      "  (rnn): LSTM(6, 64, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "iter_per_epoch:  700 128 6\n",
      "iter_losses:  (180,)\n",
      "torch.Size([128, 9, 6])\n",
      "torch.Size([128, 9, 128]) torch.Size([128, 9, 128]) torch.Size([256])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Tensor dimension is 3, expected 2 instead.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e32cf0ccc245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model params: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m iter_loss, epoch_loss = train_rnn(\n\u001b[0m\u001b[0;32m     81\u001b[0m             model1, trainData, config1, n_epochs=30, save_plots=True)\n",
      "\u001b[1;32m<ipython-input-50-e32cf0ccc245>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[1;34m(net, train_data, t_cfg, n_epochs, save_plots)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# data,label = ProcessData(feats,y_history,y_target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# 使用新的训练函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             loss = rnn_train_iteration(net, t_cfg.loss_func,\n\u001b[0m\u001b[0;32m     26\u001b[0m                                        feats, y_history, y_target)\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-b3d4669e2f97>\u001b[0m in \u001b[0;36mrnn_train_iteration\u001b[1;34m(t_net, loss_func, X, y_history, y_target)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#print('input shape: ',data1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m    \u001b[1;31m# print('pred shape: ',pred.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#pred = pred[0, :, :]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-da2e0924fce4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mchain_matmul\u001b[1;34m(*matrices)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatrices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain_matmul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensor dimension is 3, expected 2 instead."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}